Title: Data integration and big data performance using Elasticsearch - Blackcoffer Insights

HomeOur Success StoriesData integration and big data performance using ElasticsearchOur Success StoriesITData integration and big data performance using ElasticsearchByAjay Bidyarthy-July 13, 20223135Client BackgroundClient:A Leading Tech Firm in the USAIndustry Type:IT & ConsultingServices:Software, Business Solutions, ConsultingOrganization Size:200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash.Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project SnapshotsPrevious articleWeb Data ConnectorNext articleAuvik, Connectwise integration in GrafanaAjay BidyarthyRELATED ARTICLESMORE FROM AUTHORAI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content StrategyEnhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital ApplicationROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads APMOST POPULAR INSIGHTSETL PipelineOctober 6, 2019Traceability of information – Master your data capitalJune 13, 2019Impact of Indian Economy due to COVID-19April 12, 2020Future of Work: Robot, AI and AutomationJune 26, 2021Load moreRECOMMENDED INSIGHTSHow AI will impact the future of work?Integration of a product to a cloud-based CRM platformBig Data & Analytics to Bring Transparency and Good GovernanceRising IT Cities and its Impact on the Economy, Environment, Infrastructure,...